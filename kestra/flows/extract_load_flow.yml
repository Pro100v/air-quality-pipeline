id: air_quality_extract_load
namespace: air_quality

tasks:
  - id: extract_air_quality_data
    type: io.kestra.plugin.scripts.python.Script
    script: |
      import os
      import sys

      # Add the dlt_pipeline directory to the Python path
      sys.path.append('/app')

      # Import the pipeline module
      from dlt_pipeline.air_quality.pipeline import run_pipeline

      # Run the pipeline
      success = run_pipeline()

      if not success:
          sys.exit(1)

      print("Data extraction completed successfully")

  - id: load_to_bigquery
    type: io.kestra.plugin.gcp.bigquery.Load
    from: "gs://{{ env('GCS_BUCKET_NAME') }}/air-quality/year={{ now() | date('yyyy') }}/month={{ now() | date('MM') }}/day={{ now() | date('dd') }}/air_quality_{{ now() | date('yyyyMMdd') }}.parquet"
    destinationTable: "{{ env('GCP_PROJECT_ID') }}.{{ env('BIGQUERY_DATASET', 'air_quality_dataset') }}.air_quality_raw"
    format: PARQUET
    writeDisposition: WRITE_APPEND
    credentials: "{{ file('/app/secrets/credentials.json') }}"

  - id: notify_extract_load_complete
    type: io.kestra.plugin.notifications.slack.SlackIncoming
    url: "{{ env('KESTRA_WEBHOOK_URL') }}"
    message: "Air Quality Extract & Load process completed at {{ now() }}"
    dependsOn:
      - load_to_bigquery

